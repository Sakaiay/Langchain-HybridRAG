受限于大模型对输入文本的限值，必须对长文本进行切分，切分的目的是为了让语义相似的句子结合到一个chunking中，这样检索的适合可以返回这一个chunking。
LangChain中提供了很多切分的方法，但是都比较简单，比如
- **基于字符分块**:据固定字符数目以及特定的字符进行切分。
- **固定大小分块**：指定每个块的固定令牌数，通常会有一些重叠，以保持语义连贯性。
- **根据规则递归分块**：递归分块首先尝试按照一定的标准（如段落或标题）分割文本，如果分割后的文本块仍然过大，就会在这些块上重复进行分割过程，直到所有块的大小都符合要求。这种方法适用于需要将长文本细分为较小片段的场景，同时尽量保持每个块的独立性和完整性。
- **针对特定数据的分块**：这些方法尊重内容的结构和格式元素，确保语义连贯性。例如，Markdown文本可以根据标题、列表和代码块等元素进行分块，而LaTeX文本可以根据章节、小节和公式等逻辑单元进行分块。

这种方法只能作为简单的处理，因此可以使用基于相似度的方法替代。首先根据上面的基本切分方法将文本切分成一个一个句子，让后计算句子前后的相似度，设定阈值将相似度高的文本结合在一起。为了防止chunking过大，设定chunking最大长度，超过该长度就不合并。

根据相似度切分的不足：无法处理递进式的句子并且语义相似度低的句子。如阿帕奇直升机搭载了两台机枪。其中一台用于打击前方的敌人............。还有一台用于打击后面的敌人...............。上面这个例子分为三部分，本质上其实是一部分，但是基于相似度的文档切分很容易将这个句子识别为三个部分。
使用基于困惑度的切分可以缓解这个现象，
**困惑度**：将文本输入大模型中（Qwen2-7b），通过shift label后计算交叉熵损失作为困惑度，困惑度反应的是模型对这句话的理解程度，困惑度高（loss高）说明模型不太理解这句话，困惑度低（loss低）说明模型理解这句话。因此我们以困惑度低的句子作为切分点，将困惑度高的句子结合起来作为一个chunking。

### 文档加载&&文档切分中处理的点：
1. 对于表格数据怎么处理？
2. 对于常规的图文结合的文本，都是使用OCR提取图中的内容，然后和文本结合
3. 对于排版不规则的文件怎么处理？（有一些文本内容不单单是上下排版，可能是上下排版和左右排版相结合）

文档切分的指标非常重要，提升了多少？
文档切分服务于RAG，因此通常都是利用RAG的指标作为文档切分的指标。